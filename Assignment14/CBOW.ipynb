{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5906e590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sentences after preprocessing:\n",
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "['a', 'fast', 'brown', 'fox', 'leaps', 'over', 'sleeping', 'dogs']\n",
      "['the', 'dog', 'chased', 'the', 'fox', 'through', 'the', 'field']\n",
      "['foxes', 'are', 'quick', 'and', 'clever', 'animals']\n",
      "['dogs', 'are', 'loyal', 'and', 'friendly', 'companions']\n",
      "\n",
      "Vocabulary size: 24\n",
      "\n",
      "Sample training pair: (['the', 'quick', 'fox', 'jumps'], 'brown')\n",
      "Epoch 0, Loss: 96.1723\n",
      "Epoch 200, Loss: 8.0458\n",
      "Epoch 400, Loss: 2.4323\n",
      "Epoch 600, Loss: 1.2636\n",
      "Epoch 800, Loss: 0.8127\n",
      "\n",
      "âœ… Training Complete\n",
      "\n",
      "ðŸ§© Next-word predictions:\n",
      "Context: ['brown', 'fox'] â†’ Predicted next word: jumps\n",
      "Context: ['loyal', 'and'] â†’ Predicted next word: loyal\n",
      "Context: ['chased', 'the'] â†’ Predicted next word: fox\n",
      "Context: ['brown', 'fox'] â†’ Predicted next word: jumps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Define a multi-line corpus\n",
    "# -------------------------------\n",
    "corpus = \"\"\"\n",
    "the quick brown fox jumps over the lazy dog\n",
    "a fast brown fox leaps over sleeping dogs\n",
    "the dog chased the fox through the field\n",
    "foxes are quick and clever animals\n",
    "dogs are loyal and friendly companions\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Preprocess text\n",
    "# -------------------------------\n",
    "def preprocess_text(corpus):\n",
    "    corpus = corpus.lower()\n",
    "    corpus = re.sub(r'[^a-z\\s]', '', corpus)\n",
    "    sentences = [sentence.split() for sentence in corpus.strip().split('\\n') if sentence]\n",
    "    return sentences\n",
    "\n",
    "sentences = preprocess_text(corpus)\n",
    "\n",
    "print(\"âœ… Sentences after preprocessing:\")\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = sorted(set(word for sentence in sentences for word in sentence))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Generate training data (context, target)\n",
    "# -------------------------------\n",
    "def generate_training_data(sentences, window_size):\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        for i in range(window_size, len(sentence) - window_size):\n",
    "            context = []\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                if j != 0:\n",
    "                    context.append(sentence[i + j])\n",
    "            target = sentence[i]\n",
    "            data.append((context, target))\n",
    "    return data\n",
    "\n",
    "window_size = 2\n",
    "training_data = generate_training_data(sentences, window_size)\n",
    "print(\"\\nSample training pair:\", training_data[0])\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: One-hot encoding\n",
    "# -------------------------------\n",
    "def one_hot(word):\n",
    "    vec = np.zeros(vocab_size)\n",
    "    vec[word2idx[word]] = 1\n",
    "    return vec\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Initialize parameters\n",
    "# -------------------------------\n",
    "embedding_dim = 10\n",
    "W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "W2 = np.random.randn(embedding_dim, vocab_size)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Train CBOW\n",
    "# -------------------------------\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context_words, target_word in training_data:\n",
    "        # Average context word vectors\n",
    "        x = np.zeros(vocab_size)\n",
    "        for w in context_words:\n",
    "            x += one_hot(w)\n",
    "        x = x / len(context_words)\n",
    "\n",
    "        # Forward pass\n",
    "        h = np.dot(W1.T, x)\n",
    "        u = np.dot(W2.T, h)\n",
    "        y_pred = softmax(u)\n",
    "\n",
    "        # True output\n",
    "        y_true = one_hot(target_word)\n",
    "\n",
    "        # Loss\n",
    "        total_loss += -np.sum(y_true * np.log(y_pred + 1e-9))\n",
    "\n",
    "        # Backpropagation\n",
    "        e = y_pred - y_true\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.outer(x, np.dot(W2, e))\n",
    "\n",
    "        W1 -= learning_rate * dW1\n",
    "        W2 -= learning_rate * dW2\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training Complete\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7: Predict next word\n",
    "# -------------------------------\n",
    "def predict_next_word(context_words):\n",
    "    x = np.zeros(vocab_size)\n",
    "    for w in context_words:\n",
    "        if w in word2idx:\n",
    "            x += one_hot(w)\n",
    "    x = x / len(context_words)\n",
    "    \n",
    "    h = np.dot(W1.T, x)\n",
    "    u = np.dot(W2.T, h)\n",
    "    y_pred = softmax(u)\n",
    "    \n",
    "    predicted_idx = np.argmax(y_pred)\n",
    "    return idx2word[predicted_idx]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 8: Try predictions\n",
    "# -------------------------------\n",
    "test_contexts = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\"],\n",
    "    [\"dogs\", \"are\", \"loyal\", \"and\"],\n",
    "    [\"the\", \"dog\", \"chased\", \"the\"],\n",
    "    [\"a\", \"fast\", \"brown\", \"fox\"]\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ§© Next-word predictions:\")\n",
    "for ctx in test_contexts:\n",
    "    pred = predict_next_word(ctx[-window_size:])\n",
    "    print(f\"Context: {ctx[-window_size:]} â†’ Predicted next word: {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
