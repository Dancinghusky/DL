{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 id             breed\n",
      "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
      "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
      "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
      "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
      "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever\n",
      "Found 0 validated image filenames belonging to 0 classes.\n",
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:918: UserWarning: Found 10222 invalid image filename(s) in x_col=\"id\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:918: UserWarning: Found 10222 invalid image filename(s) in x_col=\"id\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The PyDataset has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     78\u001b[39m model.compile(\n\u001b[32m     79\u001b[39m     optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     80\u001b[39m     loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     81\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     82\u001b[39m )\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Step 7: Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_generator\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Step 8: Evaluate model\u001b[39;00m\n\u001b[32m     92\u001b[39m val_loss, val_acc = model.evaluate(val_generator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:311\u001b[39m, in \u001b[36mPyDatasetAdapter.get_tf_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    306\u001b[39m     batches = [\n\u001b[32m    307\u001b[39m         \u001b[38;5;28mself\u001b[39m._standardize_batch(\u001b[38;5;28mself\u001b[39m.py_dataset[i])\n\u001b[32m    308\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)\n\u001b[32m    309\u001b[39m     ]\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe PyDataset has length 0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m._output_signature = data_adapter_utils.get_tensor_spec(batches)\n\u001b[32m    314\u001b[39m ds = tf.data.Dataset.from_generator(\n\u001b[32m    315\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_iterator,\n\u001b[32m    316\u001b[39m     output_signature=\u001b[38;5;28mself\u001b[39m._output_signature,\n\u001b[32m    317\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: The PyDataset has length 0"
     ]
    }
   ],
   "source": [
    "# ✅ Install dependencies if needed\n",
    "# !pip install tensorflow pandas matplotlib --quiet\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: IMPORT LIBRARIES\n",
    "# -------------------------------\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: DEFINE DATA PATHS\n",
    "# -------------------------------\n",
    "base_dir = r\"C:\\Users\\HP\\Downloads\\datasets_dog_breed_classification\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "labels_path = os.path.join(base_dir, \"labels.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: LOAD AND FIX LABELS\n",
    "# -------------------------------\n",
    "df = pd.read_csv(labels_path)\n",
    "print(\"Before fixing:\")\n",
    "print(df.head())\n",
    "\n",
    "# Add '.jpg' to filenames if missing\n",
    "df['id'] = df['id'].apply(lambda x: x + \".jpg\")\n",
    "\n",
    "print(\"\\nAfter fixing filenames:\")\n",
    "print(df.head())\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: VERIFY FILE EXISTENCE\n",
    "# -------------------------------\n",
    "sample_path = os.path.join(train_dir, df['id'].iloc[0])\n",
    "print(\"\\nSample path check:\", sample_path)\n",
    "print(\"Exists?\", os.path.exists(sample_path))\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 5: IMAGE DATA GENERATORS\n",
    "# -------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=train_dir,\n",
    "    x_col='id',\n",
    "    y_col='breed',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    subset='training',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=train_dir,\n",
    "    x_col='id',\n",
    "    y_col='breed',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    subset='validation',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Found {train_generator.samples} training and {val_generator.samples} validation images.\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 6: LOAD PRETRAINED MODEL (VGG16)\n",
    "# -------------------------------\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "# Freeze convolutional base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 7: BUILD THE MODEL\n",
    "# -------------------------------\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(train_generator.class_indices), activation='softmax')  # number of breeds\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 8: COMPILE MODEL\n",
    "# -------------------------------\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 9: TRAIN THE MODEL\n",
    "# -------------------------------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 10: EVALUATE MODEL\n",
    "# -------------------------------\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"\\n✅ Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 11: PLOT TRAINING HISTORY\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 12: RANDOM PREDICTIONS\n",
    "# -------------------------------\n",
    "labels = list(train_generator.class_indices.keys())\n",
    "test_images = os.listdir(test_dir)\n",
    "\n",
    "for i in range(3):\n",
    "    random_img = random.choice(test_images)\n",
    "    img_path = os.path.join(test_dir, random_img)\n",
    "    img = tf.keras.utils.load_img(img_path, target_size=(224,224))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    img_array = tf.keras.utils.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    prediction = model.predict(img_array)\n",
    "    plt.title(f\"Predicted: {labels[np.argmax(prediction)]}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
